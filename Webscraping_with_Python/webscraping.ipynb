{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38064bited2af9defb114361a345112ec36e2c37",
   "display_name": "Python 3.8.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Webscraping with Python\n",
    "\n",
    "## Instructors\n",
    "\n",
    "- Scott Bailey\n",
    "- Claire Cahoon\n",
    "- Walt Gurley\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of our workshop today, we hope you'll have a sense of when and why to webscrape, and how to extract select information from websites into useable data.  \n",
    "\n",
    "## Topics\n",
    "\n",
    "- what is webscraping?\n",
    "- ethical and legal issues in webscraping\n",
    "- HTML and CSS\n",
    "- webscraping with requests-html\n",
    "\n",
    "##  Setup\n",
    "\n",
    "With this Google Colab notebook open, click the \"Copy to Drive\" button that appears in the menu bar. The notebook will then be attached to your own user account, so you can edit it in any way you like -- you can even take notes directly in the notebook.\n",
    "\n",
    "## Zoom etiquette\n",
    "\n",
    "Please make sure that your mic is muted during the workshop.\n",
    "\n",
    "## Questions during the workshop\n",
    "\n",
    "During the workshop, we have a second instructor who will be monitoring chat on Zoom. Please feel free to ask questions by chat throughout the workshop. Our second instructor will answer as able, and will aggregate questions with answers that might help everyone. \n",
    "\n",
    "At the end of each section of the workshop, the primary instructor will answer aggregated and new questions as time permits. If we aren't able to get to your question during the workshop, please follow up with us afterward. \n",
    "\n",
    "## Jupyter Notebooks and Google Colaboratory\n",
    "\n",
    "Jupyter notebooks are a way to write and run Python code in an interactive way. They're quickly becoming a standard way of putting together data, code, and written explanations or visualizations into a single document and sharing that. There are a lot of ways that you can run Jupyter notebooks, including just locally on your computer, but we've decided to use Google's Colaboratory notebook platform for this workshop.  Colaboratory is “a Google research project created to help disseminate machine learning education and research.”  If you would like to know more about Colaboratory in general, you can visit the [Welcome Notebook](https://colab.research.google.com/notebooks/welcome.ipynb).\n",
    "\n",
    "Using the Google Colaboratory platform allows us to focus on learning and writing Python in the workshop rather than on setting up Python, which can sometimes take a bit of extra work depending on platforms, operating systems, and other installed applications. If you'd like to install a Python distribution locally, though, we're happy to help. Feel free to drop by our walk-in consulting or schedule an appointment with us.\n",
    "\n",
    "https://go.ncsu.edu/dvs-request\n",
    "\n",
    "\n",
    "## Environment\n",
    "If you would prefer to use Anaconda or your own local installation of Python or Jupyter Notebooks, for this workshop you will need an environment with the following packages installed and available:\n",
    "- `pandas`\n",
    "- `requests-html`\n",
    "\n",
    "Please note that we will likely not have time during the workshop to support you with problems related to a local environment, and we do recommend using the Colaboratory notebooks if you are at all unsure.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## What is webscraping?\n",
    "\n",
    "**Question**: what types of tasks do you think of as webscraping?\n",
    "\n",
    "Webscraping is the selective retrieval of information from HTML documents on the web. Expansively, we could include the process of directed, automated retrieval of other filetypes such as PDF and CSV from web servers. \n",
    "\n",
    "Webcrawling is the automated indexing of websites, and typically involves progressive processing of a site and its links, and the repetition of this process.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Ethical concerns in webscraping\n",
    "\n",
    "First: ethics is not law, but you should be concerned with both. The legality of different types and situations in webscraping continues to be debated and decided. \n",
    "\n",
    "There are at least two things you need to check before starting to scrape a website. \n",
    "\n",
    "1. Is the content under copyright or licensed in such a way that you should not scrape it? Are there terms of service that limit your use of the site and/or its content?\n",
    "2. Does the site have a robots.txt file that circumscribes what a robot/scraper should do on the site?\n",
    "\n",
    "Further considerations:\n",
    "- **How you scrape**: Is your webscraping going to negatively impact the site, especially due to frequency of requests? Are you identifying yourself in a header when you scrape? Are you publishing your code or redistributing that? Be good citizens of the web.\n",
    "- **What you do with the data**: Are you giving correct attribution? Are you illegally or unethically redistributing content or data? \n",
    "\n",
    "There are plenty of resources online about law, ethics, and best practices around webscraping. Here are a small few further resources if you'd like to think further about these concerns:\n",
    " \n",
    "- https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01\n",
    "- https://gijn.org/2015/08/12/on-the-ethics-of-web-scraping-and-data-journalism/\n",
    "- https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/\n",
    "\n",
    "**Notice: I am not offering you legal advice on whether to scrape or not or what to scrape, just mentioning issues for consideration.**\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Common webscraping libraries\n",
    "\n",
    "There are hundreds of tutorials online about webscraping with Python, of varying quality. \n",
    "\n",
    "- [`selenium`](https://www.selenium.dev/selenium/docs/api/py/)\n",
    "- [`scrapy`](https://github.com/scrapy/scrapy)\n",
    "- [`beautifulsoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [`requests`](https://github.com/psf/requests)\n",
    "- [`urllib`](https://docs.python.org/3/library/urllib.html)\n",
    "- [`lxml`](https://github.com/lxml/lxml)\n",
    "- [`MechanicalSoup`](https://github.com/MechanicalSoup/MechanicalSoup)\n",
    "- [`requests-html`](https://github.com/psf/requests-html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Webscraping with requests-html\n",
    "\n",
    "Why `requests-html`?\n",
    "\n",
    "- Wraps commonly used libraries in a straightforward API that answers 80% of webscraping cases\n",
    "- Provides asynchronous methods\n",
    "- Provides support for dynamic sites rendered with Javascript \n",
    "\n",
    "[`requests-html` docs](https://requests.readthedocs.io/projects/requests-html/en/latest/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the particular class we need to make http requests\n",
    "# and parse the re\n",
    "from requests_html import HTMLSession"
   ]
  },
  {
   "source": [
    "# Create an instance of that class\n",
    "session = HTMLSession()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "We're going to learn first with a site set up by ScrapingHub for practicing webscraping: http://books.toscrape.com/. This site gives us well-formed, common looking HTML and CSS. It is much cleaner than most sites you might try to scrape. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a url variable for ease of use\n",
    "url = \"http://books.toscrape.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the get request to that url, and save the response in the variable r\n",
    "r = session.get(url)"
   ]
  },
  {
   "source": [
    "I've used the variable `r` to save the response we get from the server, which contains a lot of information. `requests-html` makes quite a lot of that information available to us as properties on the response. We'll take a look at a few here. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status code of the response\n",
    "# Docs on status codes: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the response headers\n",
    "# For info: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers\n",
    "r.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML object of response\n",
    "r.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML content of html object\n",
    "r.html.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text from the <html> element\n",
    "r.html.text"
   ]
  },
  {
   "source": [
    "## An interlude on HTML and CSS\n",
    "\n",
    "Why do we need to understand HTML and CSS to webscrape? Without HTML and CSS, we can't pinpoint the specific sections of the page or content we are interested in. Specifically, within HTML we need to understand `elements` or `tags`, and `attributes`. Within CSS, we definitely need to know `classes` and `ids`.\n",
    "\n",
    "Let's take a look at our sample page with our browers' developer tools. In most browsers, you can right click on a part of the page, and click 'Inspect Element' or 'Inspect' to open the dev tools. \n",
    "\n",
    "In Safari, you may need to first enable Developer Tools: Preferences -> Advanced -> \"Show Develop menu in menu bar\" "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a piece of HTML by class\n",
    "# Find returns all instances in a lis\n",
    "# If you want just the first, add first=True to your find method\n",
    "r.html.find(\".product_pod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prod = r.html.find(\".product_pod\", first=True)\n",
    "first_prod"
   ]
  },
  {
   "source": [
    "**Question**: What's the difference in type between the two `find` calls?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we find an element, we can get it's HTML\n",
    "first_prod.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can get the text from it\n",
    "first_prod.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also chain the find calls to find progressively narrower\n",
    "# elements on the page\n",
    "sidebar = r.html.find(\".sidebar\", first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find all of the genre links within the sidebar\n",
    "# Notice how the successive tags let us find nested elements\n",
    "sidebar_genres = sidebar.find(\".nav-list ul li\")\n",
    "sidebar_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have the elements, we could collect the text\n",
    "genres = [item.text for item in sidebar_genres]\n",
    "genres"
   ]
  },
  {
   "source": [
    "`requests-html` pre-collects links for you, since it's such a common and easily findable element. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the type of the absolute links\n",
    "links = r.html.absolute_links\n",
    "print(type(links))\n",
    "links"
   ]
  },
  {
   "source": [
    "**Question**: What's the difference between the absolute links and the links below? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.html.links"
   ]
  },
  {
   "source": [
    "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
    "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
    "Activity\n",
    "</p>\n",
    "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
    "On the sample page linked above, pick out some feature on the page that you want to locate. Using the developer tools in your browser, try to find something that has a clear html or css identifier. In the cell below, write code that returns just that element. \n",
    "</p>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Write code here"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## More webscraping\n",
    "\n",
    "We'll run through another example, looking at how to extract other relevant information. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Byte of Python\n",
    "url = \"https://python.swaroopch.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the nav section, and get all links within it\n",
    "nav = r.html.find(\"nav\", first=True)\n",
    "chapters = nav.absolute_links\n",
    "chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also find those links without the shortcut\n",
    "chapter_link_els = nav.find(\"a\")\n",
    "chapter_link_els"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then extract the href attribute to get the link url \n",
    "urls = [a.attrs[\"href\"] for a in chapter_link_els]\n",
    "urls"
   ]
  },
  {
   "source": [
    "These are relative links, but we could recompose the absolute url if needed to. \n",
    "\n",
    "We used the `.attrs` method on the link elements. You could use the same method with a different attribute key, things like `href`, to find any other HTML attributes. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we wanted just the chapter links, since this is a standard list, \n",
    "# we could use slicing to remove the first and last links\n",
    "urls_filtered = urls[1:-1]\n",
    "urls_filtered"
   ]
  },
  {
   "source": [
    "We've learned how to extract specific information from a webpage. Let's build on that by writing a couple of functions to help us save some of that information.\n",
    "\n",
    "Here's a standard workflow:\n",
    "- generate a list of URLs\n",
    "- iterate over those to scrape content we're interested information\n",
    "- write that content to a file that we could process in any way we want"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store data if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to create a filename from a url and a directory name\n",
    "def create_filename(url, dirname):\n",
    "    chunks = url.split(\"/\")\n",
    "    name = chunks[-1].split(\".\")[0]\n",
    "    return os.path.join(dirname, f\"{name}.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes a url, and returns the text of the first paragraph\n",
    "# Note that this is written for this specific content\n",
    "def get_para_text(url):\n",
    "    r = session.get(url)\n",
    "    try:\n",
    "        text = r.html.find(\".markdown-section\", first=True).text\n",
    "    except:\n",
    "        text = \"Missing\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(chapters))[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over chapter urls, get the text, and write it to a file\n",
    "# We have to convert the chapters set into a list to subscript it\n",
    "for url in sorted(list(chapters))[1:-1]:\n",
    "    filename = create_filename(url, \"data\")\n",
    "    text = get_para_text(url)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(text)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "source": [
    "**Question**: Why do you think that we put the `time.sleep` function here?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
    "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
    "Activity\n",
    "</p>\n",
    "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
    "TODO\n",
    "</p>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Some examples\n",
    "\n",
    "Let's look at a few concrete examples that can also show different ways of using webscraping\n",
    "\n",
    "### Scraping text and metadata\n",
    "\n",
    "#### Epistolae\n",
    "\n",
    "My first suggestion is to email the folks who run this and find out if they would be willing to provide raw XML or txt files for your research purposes. \n",
    "\n",
    "But let's see how we would scrape it if we had to."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://epistolae.ctl.columbia.edu/'\n",
    "letter_url = 'https://epistolae.ctl.columbia.edu/letter/25967.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get(letter_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_letter = r.html.find('.pane-node-field-translated-letter')[0].find('.field-items')[0].text\n",
    "translated_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_letter = r.html.find('.pane-node-field-original-letter')[0].find('.field-items')[0].text\n",
    "original_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printed_source = r.html.find('.pane-node-field-printed-source')[0].find('.field-items')[0].text\n",
    "printed_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_date = r.html.find('.pane-node-field-date')[0].find('.field-items')[0].text\n",
    "letter_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compile the data into a dictionary and create a pandas dataframe\n",
    "# At this point, we're putting together content with metadata into \n",
    "# a format that we could easily export as csv for analysis\n",
    "letter_dict = {\n",
    "    'date': [letter_date],\n",
    "    'printed-source': [printed_source],\n",
    "    'translated-letter': [translated_letter],\n",
    "    'original-letter': [original_letter]\n",
    "}\n",
    "df = pd.DataFrame(data=letter_dict)\n",
    "df"
   ]
  },
  {
   "source": [
    "If you really wanted to crawl and scrape the whole site, you would go to the table of letters, get all the links, and start scraping. You just have to handle the pagination, which in this case is tricky since it doesn't modify the url. \n",
    "\n",
    "You might have to actually spin up an instance of a headless browser and simulate a click. That takes us to a level of complexity beyond what we want to cover today though."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Downloading PDFS or CSVs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://arxiv.org/search/?query=category+theory&searchtype=all&source=header\"\n",
    "pdf_url = \"https://arxiv.org/pdf/1805.08795\"\n",
    "r = session.get(pdf_url)\n",
    "with open('math.pdf', 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "source": [
    "You could do the exact same thing with a CSV file; the key piece for either a PDF or CSV is the write occuring in binary mode, and the object written being the contents of the request rather than the html. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Scraping HTML data tables\n",
    "\n",
    "Not all tables on the web are actually table elements. We'll go through how to handle a table, and you could take the process and apply it to other HTML structures."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_url = \"https://en.wikipedia.org/wiki/Table_(information)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = session.get(table_url)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the table and see what it contains\n",
    "data = r.html.find('.wikitable', first=True)\n",
    "data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = data.find('tr')\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsts = []\n",
    "lasts = []\n",
    "ages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    if len(row.find('td')) > 0:\n",
    "        firsts.append(row.find('td')[0].text)\n",
    "        lasts.append(row.find('td')[1].text)\n",
    "        ages.append(row.find('td')[2].text)\n",
    "\n",
    "# Be aware that this is super bug prone code since it is so specific to \n",
    "# the precise data table I'm trying to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsts, lasts, ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'first_names': firsts, \"last_names\": lasts, \"ages\": ages}\n",
    "df = pd.DataFrame(data=data)\n",
    "df"
   ]
  },
  {
   "source": [
    "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
    "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
    "Activity\n",
    "</p>\n",
    "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
    "As a final activity, if you'd like, pick a site you've been wanting to scrape or have thought about. Figure out whether you could/should scrape the site. If you legally/ethically can, pick a part of a single page from the site, and scrape information from it. \n",
    "</p>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Further topics:\n",
    "\n",
    "- headless browsers\n",
    "- dynamic websites with Javascript"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}