{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_Sonification_with_Python.ipynb","provenance":[{"file_id":"1pim68sNtiLIF7H6B9mh4mYs7CM8fdClu","timestamp":1615557980426}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NPqxpRFqMKn5"},"source":["# Data sonification with Python\n","\n","## Instructors\n","\n","- Walt Gurley\n","\n","## Workshop Description\n","Data visualization is the process of using graphical elements to represent data. This workshop introduces the concept of data sonification, using characteristics of sound to represent information. Sonification can provide an alternate mode for communicating data with implications for accessibility, engagement, and discovery. Participants in this workshop will get an overview of sonification techniques and tools and learn basic processes for mapping data to sound using the Python programming language."]},{"cell_type":"markdown","metadata":{"id":"Q00n3-A8wrN4"},"source":["## Notebook setup\n","\n","This cell loads the necessary libraries and modules. If this notebook is run in Google Colab, it will also install and load extra dependencies to create audio files and play audio files. If this notebook is run on a local machine, the process of creating and playing audio is simplified and does not require the creation of audio files.\n","\n","Libraries imported into this notebook:\n","- [music21](https://web.mit.edu/music21/) - toolkit for computer-aided musicology\n","- [pandas](https://pandas.pydata.org/) - a data analysis and manipulation tool\n","- [NumPy](https://numpy.org/) - a library supporting numerical analysis\n","- [matplotlib]() - a plotting library\n","- [midi2audio](https://pypi.org/project/midi2audio/) - a tool for synthesizing or playing MIDI audio\n","\n","Additional dependencies:\n","- [Fluidsynth](http://www.fluidsynth.org/) - a synthesizer for processing MIDI files\n","- [IPython Audio controls](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html?highlight=audio#IPython.display.Audio) - a tool for playing audio and generating audio controls in a Jupyter notebook "]},{"cell_type":"code","metadata":{"tags":[],"id":"UdqdA8g9wrN5"},"source":["# Test if this notebook is running in Colab\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","print(\"I am in Colab: \" + str(IN_COLAB))\n","\n","# If running in Colab install additional dependencies to create audio files from\n","# MIDI files\n","if IN_COLAB:\n","  # Install synthesizer to provide sound fonts (i.e., instruments)\n","  !apt install fluidsynth\n","  # Copy the soundfonts file to our session storage space\n","  !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n","  # Install and load midi2audio module to convert MIDI files to audio files\n","  !pip install midi2audio\n","  from midi2audio import FluidSynth\n","\n","# Load modules from the music21 library for sonifying data\n","from music21 import instrument, note, scale, stream, midi, tempo\n","\n","# Load data processing and visualization libraries\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","plt.style.use('ggplot')\n","\n","# Load the Audio display tool to play and control audio\n","from IPython.display import Audio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1qGIoJIz_LI"},"source":["## A brief overview of audio properties and sonification\n","\n","Sound travels through air like a wave as particles are compressed together and then stretched apart. By measuring how these particles change we can represent sound as a series of waves called a waveform.\n","\n","An audio waveform has two basic properties, **amplitude** and **wavelength**. Amplitude is measured as the magnitude of displacement of a particle from its original position and can be thought of as loudness. Wavelength is used to measure frequency. Frequency is a measure of how many times the waveform repeats over time. Frequency is directly related to pitch, lower frequencies have a lower pitch and higher frequencies a higher pitch. Humans have a general hearing frequency range of 20 Hz to 20,000 Hz (Hz = one cycle per second).\n","\n","Here is a wonderful [interactive guide to audio waveforms](https://pudding.cool/2018/02/waveforms/) by Josh Comeau.\n","\n","![An image representing a sound wave traveling through the air and as a waveform.](https://github.com/WaltGurley/rem-rem-cur/blob/gh-pages/MicIn/particleSound.png?raw=true \"As a sound wave travels through the air particles are compressed and stretched apart. This can be modeled as a waveform\")\n","\n","Beyond the basic properties of sound waves we can also consider the audio properties of timbre (the perceived quality of a sound, e.g., how a guitar sounds different than a trumpet), tempo (the speed at which a collection of sounds are played, e.g., beats per minute), and spatial positioning (where a sound is coming from in space, e.g., panning left or right in stereo sound).\n","\n","Just as we may map a data value to color, position, or size on a graph, we can use these properties of sound to represent data sonically. Sonification has the ability to represent data in a way that complements visualization and has valid application with regard to accessibility, engagement, and discovery:\n","\n","- Accessibility: [SAS Graphics Accelerator](https://support.sas.com/software/products/graphics-accelerator/samples/index.html)\n","\n","- Engagement: [Sounds from Around the Milky Way](https://www.nasa.gov/mission_pages/chandra/news/data-sonification-sounds-from-around-the-milky-way.html)\n","\n","- Discovery: [Sonification of Cyclone Sidr](https://youtu.be/RRluA1r3rTk)\n"]},{"cell_type":"markdown","metadata":{"id":"gk74DIs4wrN9"},"source":["## Load and observe/clean the dataset\n","\n","We will be working with a dataset consisting of monthly atmospheric CO2 values measured at Mauna Loa Observatory, Hawaii from the Scripps CO2 program website (visit the [Scripps CO2 program website](https://scrippsco2.ucsd.edu/data/atmospheric_co2/primary_mlo_co2_record.html) for more information about the dataset we are using).\n","\n","We load the raw dataset directly from the Scripps website using the pandas method `read_csv()`. The raw data file requires some intial parsing and manipulation using arguments passed into `read_csv()`, such as identifying the starting row of the data (using the keyword argument `header=56`), filtering only the columns of data of interest (using the keyword argument `usecols=[0, 1, 2, 4, 5]`, and parsing dates (using the keyword argument `parse_dates=[2]` and the `date_parser` function). Comments are included in the code below to describe each argument."]},{"cell_type":"code","metadata":{"tags":[],"id":"KTv0U4EKwrN-"},"source":["# Read and format the csv file located at the provided URL into a Pandas DataFrame\n","co2_raw = pd.read_csv(\n","  # The URL of the csv file\n","  \"https://scrippsco2.ucsd.edu/assets/data/atmospheric/stations/in_situ_co2/monthly/monthly_in_situ_co2_mlo.csv\",\n","  # The row position of the column headers for the dataset\n","  header=56,\n","  # Create new column header names to rename given headers\n","  names=[\"year\", \"month\", \"date\", \"CO2ppm\", \"season_adj\"],\n","  # Only take the specified columns from the csv file\n","  usecols=[0,1,2,4,5],\n","  # Parse dates from the data given in column 2\n","  parse_dates=[2],\n","  # How to parse the date values\n","  date_parser=lambda x: pd.to_datetime(int(x), unit='D', origin=pd.Timestamp('01-01-1900')),\n","  # Set the column index of the DataFrame\n","  index_col=2\n",")\n","\n","# Print the data\n","co2_raw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"667sdEAqwrOB"},"source":["# Replace missing values (-99.99) with NaN\n","co2 = co2_raw.replace(-99.99, np.nan)\n","\n","# Trim dataset to remove leading and trailing missing CO2 data\n","co2 = co2.loc[co2[\"CO2ppm\"].first_valid_index():co2[\"CO2ppm\"].last_valid_index()]\n","\n","# Print the data\n","co2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rryyuA0WUTIm"},"source":["# Configure the plot\n","plt.figure(figsize=[8, 5])\n","plt.xlabel(\"Time (months)\")\n","plt.ylabel(\"CO2 ppm\")\n","plt.title(\"Monthly atmospheric CO2 values measured at Mauna Loa Observatory, Hawaii\")\n","\n","# Plot the data\n","plt.plot(co2[\"CO2ppm\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qelb72YFVKbs"},"source":["**Question:** What properties of sound could we use to sonify this dataset (e.g., frequency, amplitude, timbre, tempo, and spatial position)?"]},{"cell_type":"markdown","metadata":{"id":"bEXeBuTMwrOE"},"source":["## Audification\n","Audification is a type of sonification in which a data series is directly translated into an audio waveform. This sonification process is applied in research ranging from medicine to seismology and astronomy.\n","\n","Audification is typically suitable for large datasets that have a cyclical component.\n","\n","Examples:\n","\n","- [Vibrations of the Sun](https://soundcloud.com/nasa/sun-sonification)\n","- [Audification of brainwave data](https://youtu.be/y1Nl3De_frM)"]},{"cell_type":"markdown","metadata":{"id":"UzlygM1RSTxg"},"source":["### Audification of a sine wave\n","We will first demonstrate the process of audification by generating a sine wave over time and then converting that data into audio.\n","\n","When creating audio we need to consider two things, the sample rate and the shape of the sound wave."]},{"cell_type":"code","metadata":{"id":"36rB0aBTLzHJ"},"source":["# How many times per second (Hz) are we sampling our data?\n","sample_rate = 44100\n","\n","# Over how many seconds are we sampling our data?\n","time = 2\n","\n","# Generate a series of time samples at a rate of 'sampleRate' Hz over 'time' \n","# seconds (44100 Hz * 2 seconds = 88200)\n","time_samples = np.linspace(0, time, sample_rate * time)\n","\n","# Print out time samples\n","time_samples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oItIPoS5cpme"},"source":["# Generate a sine wave with a frequency of 'frequency' Hz over 'timeSample' samples \n","frequency = 220\n","sine_wave = np.sin(2 * np.pi * frequency * time_samples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7InH00ORcsrp"},"source":["# Configure the sine wave plot\n","plt.figure(figsize=[15, 5])\n","plt.xlabel('Time (seconds)')\n","plt.ylabel('Amplitude')\n","plt.title(f'{frequency} Hz sine wave')\n","\n","# Only plot 1/4 of the data (0.25 seconds)\n","time_max = sample_rate // 4\n","plt.plot(time_samples[:time_max], sine_wave[:time_max])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhG2kwaiEeON"},"source":["# Generate audio from sine wave data\n","Audio(sine_wave, rate=sample_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7y0Er_qM8-WV"},"source":["**Question**: Would a 440 Hz sine wave have a higher or lower pitch than the 220 Hz sine wave?"]},{"cell_type":"markdown","metadata":{"id":"V2KxY0YnsXw-"},"source":["### Audification of CO2 concentration data\n","We need to modify our data in order to create an audification. First, our data has a sample rate of 12 samples per year. That sample rate is approximately 0.0000004 Hz, and based on the lower limit of human hearing (20 Hz), this frequency is well below our ability to hear.\n","\n","To bring our dataset into an audible frequency range we must speed it up considerably. We will compress the time scale of our data to a sample rate of 3000 Hz (the lowest sample rate at which we can playback audio with the IPython Audio tool). This equates to a frequency increase of about 10^10."]},{"cell_type":"code","metadata":{"id":"sMWoQ9LtLfGq"},"source":["# Set our sample rate at 3000 Hz\n","data_sample_rate = 3000\n","\n","# Generate a series of time samples at a rate of 'dataSampleRate' Hz over 1 second\n","data_time_samples = np.linspace(0, 1, data_sample_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nT1V20UY_vY_"},"source":["# Use a linear interpolation to fill NaN values–the audio player WILL NOT WORK\n","# with data that contains any NaN values\n","co2_int = co2[\"CO2ppm\"].interpolate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_PnFimTeUAa"},"source":["# Configure the plot\n","plt.figure(figsize=[8, 5])\n","plt.xlabel('Time (seconds)')\n","plt.ylabel('Amplitude')\n","plt.title('CO2 concentration compressed to a sample rate of 3000 Hz')\n","\n","# Plot the time compress waveform of CO2 concentration data\n","plt.plot(data_time_samples[:len(co2_int)], co2_int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fm8zm69Wefml"},"source":["# The sample rate of our data is approximately 0.0000004 Hz (12 samples / year),\n","# we speed it up about 10^10 times (3000 Hz)\n","Audio(co2_int, rate=data_sample_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"88FxaUqO_IGL"},"source":["**Question:** Why is our audification so short?"]},{"cell_type":"markdown","metadata":{"id":"ivhZJCWnHVie"},"source":["### Audification of normalized CO2 concentration data\n","Even when resampling our dataset at a higher frequency, we still don't really get a useful audio representation of our waveform. We need to modify our dataset even further to establish a central value about which we can measure displacement. To do this we will normalize our dataset by removing the longterm trend from the data (i.e., subtracting the seasonally adjusted CO2 concentration values from the true CO2 concentration values)"]},{"cell_type":"code","metadata":{"id":"86OvHtAewrOH"},"source":["# Remove the longterm trend of increasing CO2 concentration\n","co2[\"fit_removed\"] = (co2[\"CO2ppm\"] - co2[\"season_adj\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lU3POvVJ_2Ez"},"source":["# Use a linear interpolation to fill NaN values–the audio player WILL NOT WORK\n","# with data that contains any NaN values\n","co2_fit_int = co2[\"fit_removed\"].interpolate()\n","\n","# Print the data\n","co2_fit_int"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKZvCW4ljJBG"},"source":["# Plot CO2 concentration data\n","plt.figure(figsize=[15, 10])\n","plt.subplot(2, 2, 1)\n","plt.xlabel('Time (months)')\n","plt.ylabel('CO2 ppm')\n","plt.title('CO2 concentration')\n","plt.plot(co2[\"CO2ppm\"])\n","\n","# Plot seasonally adjusted CO2 concentration data\n","plt.subplot(2, 2, 2)\n","plt.xlabel('Time (months)')\n","plt.ylabel('CO2 ppm')\n","plt.title('Seasonally adjusted CO2 concentration')\n","plt.plot(co2[\"season_adj\"])\n","\n","# Plot normalized CO2 concentration data over 3000 Hz sample frequency\n","plt.subplot(2, 2, 3)\n","plt.xlabel('Time (seconds)')\n","plt.ylabel('Amplitude')\n","plt.title('Normalized CO2 concentration artificially sampled at 3000 Hz')\n","plt.plot(data_time_samples[:len(co2_fit_int)], co2_fit_int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ZAQo4lnjMZz"},"source":["# The sample rate of our data is approximately 0.0000004 Hz (12 samples / year),\n","# we speed it up about 10^10 times (3000 Hz)\n","Audio(co2_fit_int, rate=data_sample_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qPZf8HifA3lr"},"source":["**Question:** Does this audification provide you with any insight into the data or help you pick out any discernable patterns?"]},{"cell_type":"markdown","metadata":{"id":"-1KTsWrUQZZ5"},"source":["## Parameter mapping\n","\n","Parameter mapping is the process of mapping data to properties of sound. As previously mentioned, these properties can include pitch (frequency), amplitude (loudness), tempo (beat speed), and timbre (quality). The spatial position of sound (for example, panning audio to the left or right channel in a stereo mix) can also be used as a mapping property.\n"," \n","As opposed to audification, mapping data to sound properties provides more options and opportunities to represent the features of a dataset.\n","\n","In this demonstration we will only explore using variations in pitch, tempo, and timbre to represent monthly atmospheric CO2 values."]},{"cell_type":"markdown","metadata":{"id":"er5glbZEwrOK"},"source":["### Subsample CO2 concentration data for sonification\n","We will subsample our data to create a smaller dataset that will be appropriate for generating shorter demo sonifications. Additionally, due to some constraints with audio playback in Google Colab we have to create audio files of our sonifications and then load them into our notebook for playback. In this case, smaller files are preferable for shorter load times."]},{"cell_type":"code","metadata":{"id":"9Ar1VxrswrOL"},"source":["# Create a subsample of the CO2 data from the year 2000 to present (co2_modern)\n","co2_modern = co2[co2[\"year\"] >= 2000]\n","\n","# Print the data\n","co2_modern"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2opdyLvLdKA"},"source":["# We are only going to be working with the measured CO2 concentration values\n","# moving forward, so we will store it in a variable (co2_ppm)\n","co2_ppm = co2_modern[\"CO2ppm\"]\n","\n","# Print the data\n","co2_ppm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fC4WQzC0wrOP"},"source":["# Plot the subsampled CO2 concentration data\n","plt.plot(co2_ppm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R20VNbUnCOv9"},"source":["# Check out some basic statistics of the subsampled data\n","co2_ppm.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jlv18L5UEIiT"},"source":["**Question:** Given the values of the descriptive statistics, could we directly use the CO2 concentration data as frequencies (in Hz) in a sonification?"]},{"cell_type":"markdown","metadata":{"id":"6uZN64XgW4qI"},"source":["### Functions for generating audio streams and creating audio files\n","\n","The function `create_audio_stream` creates a music21 audio stream that is playable in local Jupyter notebook using the call `[stream_name].show('midi')`. This functionality is not available in Google Colab, so we must also create an audio file for playback using the function `create_audio_file`.\n","\n","`create_audio_stream` takes three arguments:\n","- `notes` - a series of music21 Note objects\n","- `bpm` - beats per minute, or the speed of the sonification (default value is 120, each note is treated as a 16th note and four notes constitute one beat)\n","- `instrument_name` - the name of a synthesized instrument to play the notes (default value is \"Piano\"). A list of instrument names is available in the [music21 documentation](http://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html#module-music21.instrument)\n","\n","These three arguments provide the data mappings to frequency (the `notes` argument), tempo (the `bpm` argument), and timbre (the `instrument_name` argument)."]},{"cell_type":"code","metadata":{"id":"-GGk-mHVXwWD"},"source":["def create_audio_stream(notes, bpm=120, instrument_name='Piano'):\n","    \"\"\"\n","    Creates and returns a new stream of notes using the music21 Stream class\n","    given a series of notes, beats per minute (bpm), and an instrument name\n","    (instrumentName). For a list of valid instrument names see the music21\n","    Instrument module documentation https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n","    \"\"\"\n","    # Create a new music21 stream object to add notes to\n","    new_stream = stream.Stream()\n","\n","    # Set the tempo string of the stream\n","    new_stream.append(tempo.MetronomeMark(number=bpm))\n","\n","    # Set the instrument to play the stream notes\n","    new_stream.append(getattr(instrument, instrument_name)())\n","\n","    # Iterate over the notes provided in the series\n","    for this_note in notes:\n","        # Append the note to the new stream\n","        new_stream.append(this_note)\n","        # Set the duration of the note as a sixteenth note (0.25 of quarter note)\n","        this_note.duration.quarterLength = 0.25\n","    # Return a music21 stream object\n","    return new_stream"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkBAJbWCYFOB"},"source":["def create_audio_file(stream, file_name):\n","    \"\"\"\n","    Create an audio file using the given music21 stream and the given file_name.\n","    It writes a MIDI file using music21 and then uses the newly written MIDI file\n","    to create the specified audio file using FluidSynth midi to audio conversion\n","    \"\"\"\n","    # Use the music21 stream object write function to create a MIDI file\n","    stream.write('midi', file_name.split('.')[0] + '.mid')\n","\n","    # Use the FluidSynth module to call the sound font and convert the newly \n","    # created MIDI file to the specified file_name using the midi_to_audio function\n","    FluidSynth('font.sf2').midi_to_audio(file_name.split('.')[0] + '.mid', file_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WQcth6LrwrOR"},"source":["### Parameter mapping using CO2 concentration data values as frequency (pitch)\n","\n","In this demonstration we will use a function that directly maps a numeric value to a frequency and the corresponding scientific pitch notation value.\n","\n","For example, a data value of 440 would be interpreted as the frequency 440 Hz. This frequency would then be converted to scientific pitch notation as the pitch A4 (A in octave 4).\n","\n","Moving forward we will be thinking of frequency in terms of Hz and scientific pitch notation. It might be helpful to reference this [table of piano key pitches and frequencies](https://en.wikipedia.org/wiki/Piano_key_frequencies#List)."]},{"cell_type":"markdown","metadata":{"id":"yxC5HdyPYDPC"},"source":["We will use the defined function `translate_value_to_pitch` to translate data values as frequency to pitch. This function takes one argument:\n","\n","- `value_as_frequency` - a data value interpreted as a frequency"]},{"cell_type":"code","metadata":{"id":"yhuk3043wrOU"},"source":["def translate_value_to_pitch(value_as_frequency):\n","    \"\"\"\n","    Translate a raw data value as frequency to a corresponding\n","    pitch. It returns the pitch of the provided frequency value if the value\n","    is greater than zero. If the value is less than zero it returns a rest (i.e., \n","    silence).\n","    \"\"\"\n","    # Test if the frequency value is greater than zero\n","    if (value_as_frequency > 0):\n","        # Create a music21 Note object\n","        converted_note = note.Note()\n","        # Set the pitch of the Note object based on the supplied frequency value\n","        converted_note.pitch.frequency = value_as_frequency\n","        # Return the Note object with the assigned pitch\n","        return converted_note\n","    # Set the Note to a Rest value (silence)\n","    converted_note = note.Rest()\n","    # Return the Note object with the assigned pitch\n","    return converted_note"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnNguBMc4PmW"},"source":["# Test the translate_value_to_pitch function\n","test_note = translate_value_to_pitch(440)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PPc2WNAxMGKD"},"source":["The `translateValueToPitch` function has returned a [music21 Note object](http://web.mit.edu/music21/doc/moduleReference/moduleNote.html#module-music21.note). A Note object has many properties. For this workshop the most important properties are the note name and pitch frequency. We can get a full desciptive name by calling the property `fullName` on a Note object. We can get the frequency of a note by callinf the property `pitch.frequency` on a Note object."]},{"cell_type":"code","metadata":{"id":"zaxkfC8FvbRr"},"source":["# Print out full note name information (fullName)\n","test_note.fullName"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuV6k2SD5-g6"},"source":["# Print out the frequency information using \"pitch.frequency\" (test_note)\n","test_note.pitch.frequency"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"80rVTGkawrOW"},"source":["# Map CO2 concentration values directly to pitch\n","raw_pitch = co2_ppm.apply(translate_value_to_pitch)\n","\n","#Print the data\n","raw_pitch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sP72yepSwrOY"},"source":["# Create an audio stream of the raw_pitch values using the create_audio_stream\n","# function\n","# Pass in the series of notes (raw_pitch), a tempo in bpm, and an instrument name\n","# from the list located in the right-hand column of this music21 documentation\n","# page: https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n","raw_pitch_stream = create_audio_stream(raw_pitch, 120, 'Xylophone')\n","\n","# If running on a local machine, uncomment the line below to play the MIDI data\n","# in the Jupyter notebook without having to create a file\n","# raw_pitch_stream.show('midi')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i5Omr_loyux6"},"source":["In the next cell we are creating an audio file from the music21 stream we created. We will create a FLAC audio file, but it is also possible to create other standard audio file formats such as mp3, mp4, and wave files by changing the file extension. When running this notebook in Colab I found that FLAC audio files tended to load faster with than other file types when using the Audio tool. "]},{"cell_type":"code","metadata":{"id":"rJ66bB6UU316"},"source":["# Create an audio file from the raw_pitch_stream\n","create_audio_file(raw_pitch_stream, 'raw_pitch.flac')\n","\n","# Load the newly created audio file for playback\n","Audio('raw_pitch.flac')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJ0tcDEeyoGy"},"source":["# Plot the data for visual reference\n","plt.plot(co2_ppm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HCZWd0-QUcli"},"source":["#### Activity: Sonify seasonally adjusted CO2 concentrations using `translate_value_to_pitch`\n","\n","Create a sonification of the seasonally adjusted CO2 concentration data from the `co2_modern` dataset using the `translate_value_to_pitch` method."]},{"cell_type":"code","metadata":{"id":"8UH95jGTTa-P"},"source":["# Store the seasonally adjusted concentration data from \"co2_modern\" in a variable\n","\n","\n","# Use the apply() method to call translate_value_to_pitch on each row of the\n","# seasonally adjusted concentration data\n","\n","\n","# Create the audio stream from the pitch data\n","\n","\n","# Create the audio file from the audio stream\n","\n","\n","# Load the audio file for playback\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7slkEh5YYBg"},"source":["**Question:** Are there any shortcomings with the method we used to create this sonification?"]},{"cell_type":"markdown","metadata":{"id":"rueqdiqmwrOc"},"source":["### Sonification using values mapped to linear pitch range\n","In this demonstration we will use a function that maps a data value from the dataset range (dataMin - dataMax) to the pitch range of a standard 88 key piano (~27 Hz – ~4186 Hz)."]},{"cell_type":"markdown","metadata":{"id":"t6F2Nh9Igz9U"},"source":["We will use the defined function `map_value_to_pitch_range` to map data values from the data domain to a pitch range. This function takes two arguments:\n","- `data_value` - the data value to be mapped to the new pitch range\n","- `data_min_max` - a list containing the min and max of the dataset ([min, max])"]},{"cell_type":"code","metadata":{"id":"z93Dh_EEwrOt"},"source":["def map_value_to_pitch_range(data_value, data_min_max):\n","    \"\"\"\n","    Map a data value from the dataset domain (data min - data max)\n","    to the frequency range of a standard 88 key piano (~16 Hz - ~4186 Hz). It\n","    returns a music21 Note object with the pitch of the data value mapped to the\n","    frequency range.\n","    \"\"\"\n","    # Create the music21 Note object\n","    converted_note = note.Note()\n","\n","    # Set the data range using supplied min and max\n","    data_range = data_min_max[1] - data_min_max[0]\n","\n","    # Frequency of the lowest note on a standard 88 key piano\n","    MIN_HZ = 27.50\n","    # Frequency of the highest note on a standard 88 key piano\n","    MAX_HZ = 4186.01\n","\n","    # Set the pitch of each Note object mapping from the data range to the\n","    # frequency range of a standard 88 key piano\n","    converted_note.pitch.frequency = (\n","        ((data_value - data_min_max[0]) * (MAX_HZ - MIN_HZ)) / data_range\n","    ) + MIN_HZ\n","    return converted_note"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RVUWtlLn8GdE"},"source":["# Test the map_value_to_pitch_range function\n","map_value_to_pitch_range(50, [1, 100]).pitch.frequency"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhEaI1pawrOv"},"source":["# Calculate the minimum and maximum values of the CO2 concentration data\n","co2_range = co2_ppm.agg(['min', 'max'])\n","\n","# Convert values to pitch range\n","pitch_range = co2_ppm.apply(\n","  lambda x: map_value_to_pitch_range(x, [co2_range['min'], co2_range['max']])\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzKaNhYIwrOy"},"source":["# Create an audio stream of the pitch_range values using the create_audio_stream\n","# function\n","# Pass in the series of notes (pitch_range), a tempo in bpm, and an instrument name\n","# from the list located in the right-hand column of this music21 documentation\n","# page: https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n","pitch_range_stream = create_audio_stream(pitch_range, instrument_name='Glockenspiel')\n","\n","# If running on a local machine, uncomment the line below to play the MIDI data\n","# in the Jupyter notebook without having to create a file\n","# pitch_range_stream.show('midi')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0NAe1HKfe6HG"},"source":["# Create an audio file from the pitch_range_stream\n","create_audio_file(pitch_range_stream, 'pitch_range.flac')\n","\n","# Load the newly created audio file for playback\n","Audio('pitch_range.flac')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3zuCH3KmdvTp"},"source":["# Plot the actual concentration data and the mapped frequency data for visual reference\n","plt.figure(figsize=[12, 5])\n","plt.subplot(1, 2, 1)\n","plt.title('CO2 concentration over time')\n","plt.plot(co2_ppm)\n","\n","plt.subplot(1, 2, 2)\n","plt.title('CO2 concentration mapped to\\n frequency over time (seconds)')\n","# Create the time scale for the audio file using the bpm\n","time_scale = np.linspace(0, (len(pitch_range) / 4) / 120 * 60, len(pitch_range))\n","plt.plot(time_scale, pitch_range.apply(lambda x: x.pitch.frequency))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GrpJOvczUYVa"},"source":["#### Activity: Sonify seasonally adjusted CO2 concentrations using `map_value_to_pitch_range`\n","\n","Create a sonification of the seasonally adjusted CO2 concentration data using the `map_value_to_pitch_range` method."]},{"cell_type":"code","metadata":{"id":"dkfjRYl9VF7N"},"source":["# Calculate the min and max values of the seasonally adjusted C02 concentration\n","# data\n","\n","\n","# Use the apply() method to call map_value_to_pitch_range on each row of the\n","# seasonally adjusted concentration data\n","\n","\n","# Create the audio stream from the pitch data\n","\n","\n","# Create the audio file from the audio stream\n","\n","\n","# Load the audio file for playback\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvZvi4i-cnpn"},"source":["### Sonification using values mapped to an exponential pitch range\n","The frequency ranges between pitches are not linear. For example, the frequency of note A4 is 440 Hz. To get to the next octave of this note we must double the frequency of A4, giving A5 as 880 Hz (i.e., A6 = 1760, A3 = 220, and so on). In order to map our values to an appropriate pitch scale, we can apply a log base two transform to pitch range. This makes it easier to discern pitches at lower frequencies and produces a more natural and pleasing sound.\n","\n","In this demonstration we will use a function that maps a data value from the dataset range (dataMin - dataMax) to the frequency range of a standard 88 key piano (\\~27 Hz – \\~4186 Hz) over an exponential scale."]},{"cell_type":"markdown","metadata":{"id":"gaYvm_QxhTXr"},"source":["We will use the defined function `map_value_to_pitch_range_exp_scale` to map data values from the data domain to a log scale pitch range. This function takes two arguments:\n","- `data_value` - the data value to be mapped to the new pitch range\n","- `data_min_max` - a list containing the min and max of the dataset ([min, max])"]},{"cell_type":"code","metadata":{"id":"zvu14U2ewrOm"},"source":["# https://stackoverflow.com/questions/19472747/convert-linear-scale-to-logarithmic\n","#           x - x0\n","# log(y) = ------- * (log(y1) - log(y0)) + log(y0)\n","#          x1 - x0\n","def map_value_to_pitch_range_exp_scale(data_value, data_min_max):\n","    \"\"\"\n","    Map a data value from the dataset domain (data min - data\n","    max) to the log2 frequency range of a standard 88 key piano (~16 Hz – ~4186\n","    Hz). It returns a music21 Note object with the pitch of the mapped frequency\n","    value.\n","    \"\"\"\n","    # Create the music21 Note object\n","    converted_note = note.Note()\n","\n","    # Set the data range using supplied min and max\n","    data_range = data_min_max[1] - data_min_max[0]\n","\n","    # Frequency of the lowest note on a standard 88 key piano\n","    MIN_HZ = 27.50\n","    # Frequency of the highest note on a standard 88 key piano\n","    MAX_HZ = 4186.01\n","\n","    # Set the pitch of each Note object, mapping from the data range to a\n","    # log base 2 frequency range of a standard 88 key piano\n","    converted_note.pitch.frequency = np.exp2(\n","        (data_value - data_min_max[0]) / data_range *\n","        (np.log2(MAX_HZ) - np.log2(MIN_HZ)) +\n","        np.log2(MIN_HZ)\n","    )\n","    return converted_note"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBK8oSmrTAp2"},"source":["# Test the map_value_to_pitch_range function\n","map_value_to_pitch_range_exp_scale(0, [-50, 50]).pitch.frequency"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nYRAPzkwrOo"},"source":["# Convert values to pitch range\n","pitch_range_exp = co2_ppm.apply(lambda x:\n","  map_value_to_pitch_range_exp_scale(x, [co2_range['min'], co2_range['max']])\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfnHYcCCwrOq"},"source":["# Create an audio stream of the pitch_range_exp values using the createAudioStream\n","# function. Pass in the series of notes (pitch_range_exp), a tempo in bpm, and an\n","# instrument name from the list located in the right-hand column of this music21\n","# documentation page: https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n","pitch_range_exp_stream = create_audio_stream(pitch_range_exp, 100, 'Xylophone')\n","\n","# If running on a local machine, uncomment the line below to play the MIDI data\n","# in the Jupyter notebook without having to create a file\n","# pitch_range_exp_stream.show('midi')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k7eFc7kufKed"},"source":["# Create an audio file from the pitch_range_exp_stream\n","create_audio_file(pitch_range_exp_stream, 'pitch_range_exp.flac')\n","\n","# Load the newly created audio file for playback\n","Audio('pitchR_range_exp.flac')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFlUrzq_b_MI"},"source":["# Plot the actual concentration data and the mapped frequency data for visual reference\n","plt.figure(figsize=[12, 5])\n","plt.subplot(1, 2, 1)\n","plt.title('CO2 concentration over time')\n","plt.plot(co2_ppm)\n","\n","plt.subplot(1, 2, 2)\n","plt.title('CO2 concentration mapped to frequency over time')\n","# Calculate the time scale for the audio we generated using the bpm\n","time_scale = np.linspace(0, len(pitch_range) / 4 / 100 * 60, len(pitch_range))\n","\n","plt.plot(time_scale, pitch_range_exp.apply(lambda x: x.pitch.frequency))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2QkwZQ-UTlq"},"source":["#### Activity: Sonify seasonally adjusted CO2 concentrations using `map_value_to_pitch_range_exp_scale`\n","\n","Create a sonification of the seasonally adjusted CO2 concentration data using the `map_value_to_pitch_range_exp_scale` method."]},{"cell_type":"code","metadata":{"id":"0qYPT9xgVb7r"},"source":["# Use the apply() method to call map_value_to_pitch_range on each row of the\n","# seasonally adjusted concentration data\n","\n","\n","# Create the audio stream from the pitch data\n","\n","\n","# Create the audio file from the audio stream\n","\n","\n","# Load the audio file for playback\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u7DhEXAXwrO3"},"source":["### Sonification using values mapped to a musical scale\n","\n","In this demonstration we will use a function that maps a data value from the dataset range (dataMin - dataMax) to a specified [musical scale](https://en.wikipedia.org/wiki/Scale_(music)) (i.e., an ordered group of pitches) with the given provided starting note (i.e., the tonic) and an octave range (i.e., the number of notes over which to map your data).\n","\n","Using a musical scale provides us with some options to make our sonification more \"musical\". Previously we were mapping values to frequencies and pitches without consideration for ordering of notes. Predefined musical scales can evoke culturally influenced emotions that can give our sonification an aesthetic feeling. For example, major scales are often interpreted as happy, while minor scales are interpreted as sad."]},{"cell_type":"markdown","metadata":{"id":"FQblzbv6h8AN"},"source":["We will use the defined function `map_value_to_scale` to map data values from the data domain to a musical scale. This function takes five arguments:\n","- `data_value` - the data value to be mapped to the new pitch range\n","- `data_min_max` - a list containing the min and max of the dataset ([min, max])\n","- `tonic` - a string containing a valid scientific pitch notation for the first note of the scale\n","- `scale_name` - a string containing a valid scale subclass name from the music21 scale class (default value is \"MajorScale\"). A list of scale names is available in the [music21 documentation](http://web.mit.edu/music21/doc/moduleReference/moduleScale.html#module-music21.scale)\n","- `octave_range` - a list containing the min and max octave numbers over which to map the pitches (default value is [3, 5])"]},{"cell_type":"code","metadata":{"id":"-TIcOeWuwrO3"},"source":["def map_value_to_scale(data_value, data_min_max, tonic='c',\n","                       scale_name='MajorScale', octave_range=[3, 5]):\n","    \"\"\"\n","    Map a data value from the dataset domain (data min - data max)\n","    to a musical scale with a provided tonic note over a provided octave range. It\n","    returns a music21 Note object with the pitch of the mapped data value. \n","    \"\"\"\n","    # Create the music21 Note object\n","    converted_note = note.Note()\n","\n","    # Set the scale using the supplied tonic value\n","    this_scale = getattr(scale, scale_name)(tonic)\n","    # Get the pitches of the scale based on the octave range\n","    scale_pitches = this_scale.getPitches(\n","        tonic + str(octave_range[0]),\n","        tonic + str(octave_range[1]))\n","    \n","    # Get the data range using supplied min and max \n","    dataRange = data_min_max[1] - data_min_max[0]\n","\n","    # Get the position of the note on the scale given the range of the scale\n","    note_position = round(\n","        ((data_value - data_min_max[0]) * (len(scale_pitches) - 1)) / dataRange)\n","    # Set the pitch of the note\n","    converted_note.pitch = scale_pitches[note_position]\n","    return converted_note"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9WZxcng2VQB"},"source":["The C major scale has a base note (tonic) of C and consists of the notes: C, D, E, F, G, A, B."]},{"cell_type":"code","metadata":{"id":"cKNHwhlN2GjZ"},"source":["# Test the map_value_to_scale function\n","c_major_note = map_value_to_scale(0, [0, 7], octave_range=[3,4])\n","c_major_note.fullName"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGPC7-mdwrO5"},"source":["# Convert values to musical scale\n","musical_notes = co2_ppm.apply(lambda x:\n","  map_value_to_scale(\n","    x,\n","    [co2_range['min'], co2_range['max']],\n","    tonic='d',\n","    scale_name='MinorScale',\n","    octave_range=[2, 7]\n","  )\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"5UvzyVlawrO7"},"source":["# Create an audio stream of the musical_notes values using the createAudioStream\n","# function. Pass in the series of notes (musical_notes), a tempo in bpm, and an\n","# instrument name from the list located in the right-hand column of this music21\n","# documentation page: https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n","musical_notes_stream = create_audio_stream(musical_notes, 100, 'Harpsichord')\n","\n","# If running on a local machine, uncomment the line below to play the MIDI data\n","# in the Jupyter notebook without having to create a file\n","# musical_notes_stream.show('midi')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJiYGjeDdyxo"},"source":["# Create an audio file from the musical_notes_stream\n","create_audio_file(musical_notes_stream, 'musical_notes.flac')\n","\n","# Load the newly created audio file for playback\n","Audio('musical_notes.flac')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pfyDmgPhwrO9"},"source":["# Plot the actual concentration data and the mapped frequency data for visual reference\n","plt.figure(figsize=[12, 5])\n","plt.subplot(1, 2, 1)\n","plt.title('CO2 concentration over time')\n","plt.plot(co2_ppm)\n","\n","plt.subplot(1, 2, 2)\n","plt.title('CO2 concentration mapped to frequency over time')\n","# Create the time scale for the audio file using the bpm\n","time_scale = np.linspace(0, (len(pitch_range) / 4) / 120 * 60, len(pitch_range))\n","\n","plt.plot(time_scale, musical_notes.apply(lambda x: x.pitch.frequency))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gxikh7JcUOcj"},"source":["#### Activity: Sonify seasonally adjusted CO2 concentrations using `map_value_to_scale`\n","\n","Create a sonification of the seasonally adjusted CO2 concentration data using the `map_value_to_scale` method."]},{"cell_type":"code","metadata":{"id":"4CxWBLuHVmrf"},"source":["# Use the apply() method to call map_value_to_scale on each row of the\n","# seasonally adjusted concentration data\n","\n","\n","# Create the audio stream from the pitch data\n","\n","\n","# Create the audio file from the audio stream\n","\n","\n","# Load the audio file for playback\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BA_c8TCS4ZiU"},"source":["### Sonification playground\n","\n","Use the cells below to generate your own sonifications of the CO2 concentration data using the provided functions.\n","\n","You can the call the following functions on a Pandas Series using the `apply()` function:\n","\n","- `translate_value_to_pitch(value_as_frequency)`\n","\n","- `map_value_to_pitch_range(data, data_min_max)`\n","\n","- `map_value_to_pitch_range_exp_scale(data, data_min_max)`\n","\n","- `map_value_to_scale(data, data_min_max, tonic=\"c\", scale_name=\"MajorScale\",\n","  octave_range=[3, 5])`\n","\n","For example, to create a sonification of the modern CO2 data you could do the following:\n","\n","1. Map the data to a log scale pitch range to return a series of music21 Note objects\n","```python\n","pitch_range = co2_modern['season_adj'].apply(lambda x:\n","      map_value_to_pitch_range_exp_scale(x, [co2_range['min'], co2_range['max']])\n",")\n","```\n","\n","1. Create a music21 stream of the Note objects at 100 bpm using a Guitar sound\n","```python\n","notes_stream = create_audio_stream(pitch_range, 100, 'Guitar')\n","```\n","\n","1. Create an audio file from the stream and play it back\n","```python\n","create_audio_file(notes_stream, 'demo_notes.flac')\n","Audio('demo_notes.flac')\n","```"]},{"cell_type":"markdown","metadata":{"id":"7K6a5vD5VxeK"},"source":["## Other resources\n","\n","### Completed version of this notebook\n","\n","[Data Sonification with Python filled notebook](https://colab.research.google.com/github/NCSU-Libraries/data-viz-workshops/blob/master/Data_Sonification_with_Python/Data_Sonification_with_Python_filled.ipynb) - a version of this notebook with all code filled in.\n","\n","### Sonification development tools and applications\n","\n","- [Sonic Pi](https://sonic-pi.net/) - a code-based music creation and performance tool based on the Ruby programming language\n","\n","- [p5.js](https://p5js.org/) - a JavaScript library for creative coding that includes a library for interfacing with web-based audio.\n","\n","- [Web audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) - base API for controling audio on the web\n","\n","- [SAS Graphics Accelerator](https://support.sas.com/software/products/graphics-accelerator/samples/index.html) - a free Chrome extension that allows you to sonify data captured from web tables and data from web-based SAS visualization\n","\n","- [TwoTone](https://app.twotone.io/) - An interactive web application for easily creating sonifications with uploaded data\n","\n","### Sonification theory and practice\n","\n","- [The Sonification Handbook](https://sonification.de/handbook/) - an open access textbook published in 2011 providing an introduction to sonification and sonification research and techniques\n","\n","- [An Overview of Auditory Displays and Sonification](https://sonification.de/) - the website of Thomas Hermann, one of the editors of The Sonification Handbook, providing an overview of sonification\n","\n","- [DataSonificationBlog](https://www.saralenzi.com/datasonificationblog) - blog of Sara Lenzi, sonification researcher\n","\n","- [Intentionality and design in the data sonification of social issues](https://www.researchgate.net/publication/343692618_Intentionality_and_design_in_the_data_sonification_of_social_issues) - a journal article analyzing the role of intentionality when designing sonifications that communicate social issues to public audiences\n","\n","- [Thirteen Years of Reflection on Auditory Graphing: Promises, Pitfalls, and Potential New Directions](https://digitalcommons.unl.edu/cgi/viewcontent.cgi?referer=http://playitbyr.org/&httpsredir=1&article=1429&context=psychfacpub) - a 2005 journal article covering sonification methods and discussing the success of these methods\n","\n","- [Loud Numbers](https://www.loudnumbers.net/) - a data sonification podcast and mailing list\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MAAF-mLyRqeE"},"source":["## Evaluation Survey\r\n","Please, spend 1 minute answering these questions that help improve future workshops.\r\n","\r\n","https://go.ncsu.edu/dvs-eval"]},{"cell_type":"markdown","metadata":{"id":"i6Sj-7OYR5JL"},"source":["## Credits\r\n","\r\n","This workshop was created by Walt Gurley at the NC State University Libraries."]}]}