{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataSonificationWithPython.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPqxpRFqMKn5"
      },
      "source": [
        "# Data sonification with Python\n",
        "\n",
        "## Instructors\n",
        "\n",
        "- Walt Gurley\n",
        "- Scott Bailey\n",
        "\n",
        "## Workshop Description\n",
        "Data visualization is the process of using graphical elements to represent data. This workshop introduces the concept of data sonification, using characteristics of sound to represent information. Sonification can provide an alternate mode for communicating data with implications for accessibility, engagement, and discovery. Participants in this workshop will get an overview of sonification techniques and tools and learn basic processes for mapping data to sound using the Python programming language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q00n3-A8wrN4"
      },
      "source": [
        "## Notebook setup\n",
        "\n",
        "This cell loads the necessary libraries and modules. If this notebook is run in Google Colab, it will also install and load extra dependencies to create audio files and play audio files. If this notebook is run on a local machine, the process of creating and playing audio is simplified and does not require the creation of audio files.\n",
        "\n",
        "Libraries imported into this notebook:\n",
        "- [music21](https://web.mit.edu/music21/) - toolkit for computer-aided musicology\n",
        "- pandas\n",
        "- numpy\n",
        "- matplotlib\n",
        "\n",
        "Additional dependencies:\n",
        "- [Fluidsynth](http://www.fluidsynth.org/) - a synthesizer for processing MIDI files\n",
        "- [IPython Audio controls](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html?highlight=audio#IPython.display.Audio) - a tool for playing audio and generating audio controls in a Jupyter notebook "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "UdqdA8g9wrN5"
      },
      "source": [
        "# Test if this notebook is running in Colab\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "print(\"I am in Colab: \" + str(IN_COLAB))\n",
        "\n",
        "# If running in Colab install additional dependencies to create audio files from\n",
        "# MIDI files\n",
        "if IN_COLAB:\n",
        "  # Install synthesizer to provide sound fonts (i.e., instruments)\n",
        "  !apt install fluidsynth\n",
        "  # Copy the soundfonts file to our session storage space\n",
        "  !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "  # Install and load midi2audio module to convert MIDI files to audio files\n",
        "  !pip install midi2audio\n",
        "  from midi2audio import FluidSynth\n",
        "\n",
        "# Load modules from the music21 library for sonifying data\n",
        "from music21 import instrument, note, scale, stream, midi, tempo\n",
        "\n",
        "# Load data processing and visualization libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load the Audio display tool to play and control audio\n",
        "from IPython.display import Audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1qGIoJIz_LI"
      },
      "source": [
        "## A brief overview of audio properties and sonification\n",
        "Sound travels through air like a wave as particles are compressed together and then stretched apart. By measuring how these particles change we can represent sound as a series of waves called a waveform. An audio waveform has two basic properties, amplitude and wavelength. Amplitude is measured as displacement over time and can be thought of as loudness. Wavelength is used to measure frequency. Frequency is a measure of how many times the waveform repeats over time. Frequency is directly related to pitch, lower frequencies have a lower pitch and higher frequencies a higher pitch. Humans have a general hearing frequency range of 20 Hz to 20,000 Hz (Hz = one cycle per second).\n",
        "\n",
        "Here is a great [interactive guide to audio waveforms](https://pudding.cool/2018/02/waveforms/).\n",
        "\n",
        "![An image representing a sound wave traveling through the air and as a waveform.](https://github.com/WaltGurley/rem-rem-cur/blob/gh-pages/MicIn/particleSound.png?raw=true \"As a sound wave travels through the air particles are compressed and stretched apart. This can be modeled as a waveform\")\n",
        "\n",
        "Beyond the basic properties of sound waves we can also consider the audio properties of timbre (the perceived quality of a sound, e.g., how a guitar sounds different than a trumpet), tempo (the speed at which a collection of sounds are played, e.g., beats per minute), and spatial positioning (where a sound is coming from in space, e.g., panning left or right in stereo sound).\n",
        "\n",
        "Just as we may map a data value to color, position, or size on a graph, we can use these properties of sound to represent data sonically. Sonification has the ability to represent data in a way that complements visualization and has valid application with regard to accessibility, engagement, and discovery:\n",
        "\n",
        "- Accessibility: [SAS Graphics Accelerator](https://support.sas.com/software/products/graphics-accelerator/samples/index.html)\n",
        "\n",
        "- Engagement: [Sounds from Around the Milky Way](https://www.nasa.gov/mission_pages/chandra/news/data-sonification-sounds-from-around-the-milky-way.html)\n",
        "\n",
        "- Discovery: [Sonification of Cyclone Sidr](https://youtu.be/RRluA1r3rTk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk74DIs4wrN9"
      },
      "source": [
        "## Load and observe/clean dataset\n",
        "\n",
        "We will be working with a dataset consisting of monthly atmospheric CO2 values measured at Mauna Loa Observatory, Hawaii from the Scripps CO2 program website.\n",
        "\n",
        "For Scripps CO2 program data information see: https://scrippsco2.ucsd.edu/data/atmospheric_co2/primary_mlo_co2_record.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "KTv0U4EKwrN-"
      },
      "source": [
        "# Read and format the csv file located at the provided URL into a Pandas DataFrame\n",
        "co2raw = pd.read_csv(\n",
        "  # The URL of the csv file\n",
        "  \"https://scrippsco2.ucsd.edu/assets/data/atmospheric/stations/in_situ_co2/monthly/monthly_in_situ_co2_mlo.csv\",\n",
        "  # The row position of the column headers for the dataset\n",
        "  header=56,\n",
        "  # Create new column header names to rename given headers\n",
        "  names=[\"year\", \"month\", \"date\", \"CO2ppm\", \"season_adj\"],\n",
        "  # Only take the specified columns from the csv file\n",
        "  usecols=[0,1,2,4,5],\n",
        "  # Parse dates from the data given in column 2\n",
        "  parse_dates=[2],\n",
        "  # How to parse the date values\n",
        "  date_parser=lambda x: pd.to_datetime(int(x), unit='D', origin=pd.Timestamp('01-01-1900')),\n",
        "  # Set the column index of the DataFrame\n",
        "  index_col=2\n",
        ")\n",
        "\n",
        "# Print the data\n",
        "co2raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "667sdEAqwrOB"
      },
      "source": [
        "# Replace missing values (-99.99) with NaN\n",
        "co2 = co2raw.replace(-99.99, np.nan)\n",
        "\n",
        "# Trim dataset to remove leading and trailing missing CO2 data\n",
        "co2 = co2.loc[co2[\"CO2ppm\"].first_valid_index():co2[\"CO2ppm\"].last_valid_index()]\n",
        "\n",
        "# Print the data\n",
        "co2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rryyuA0WUTIm"
      },
      "source": [
        "# Configure the plot\n",
        "plt.figure(figsize=[8, 5])\n",
        "plt.xlabel(\"Time (months)\")\n",
        "plt.ylabel(\"CO2 ppm\")\n",
        "plt.title(\"Monthly atmospheric CO2 values measured at Mauna Loa Observatory, Hawaii\")\n",
        "\n",
        "# Plot the data\n",
        "plt.plot(co2[\"CO2ppm\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qelb72YFVKbs"
      },
      "source": [
        "**Question:** What properties of sound could we use to sonify this dataset (e.g., frequency, amplitude, timbre, tempo, and spatial position)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEXeBuTMwrOE"
      },
      "source": [
        "## Audification\n",
        "Audification is a type of sonification in which a data series is directly translated into an audio waveform. This sonification process is applied in research ranging from medicine to seismology and astronomy.\n",
        "\n",
        "Audification is typically suitable for large datasets that have a cyclical component.\n",
        "\n",
        "Examples:\n",
        "\n",
        "- [Vibrations of the Sun](https://soundcloud.com/nasa/sun-sonification)\n",
        "- [Audification of brainwave data](https://youtu.be/y1Nl3De_frM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzlygM1RSTxg"
      },
      "source": [
        "### Audification of a sine wave\n",
        "We will first demonstrate the process of audification by generating a sine wave over time and then converting that data into audio.\n",
        "\n",
        "When creating audio we need to consider two things, the sample rate and the shape of the sound wave."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36rB0aBTLzHJ"
      },
      "source": [
        "# How many times per second (Hz) are we sampling our data?\n",
        "sampleRate = 44100\n",
        "\n",
        "# Over how many seconds are we sampling our data?\n",
        "time = 2\n",
        "\n",
        "# Generate a series of time samples at a rate of 'sampleRate' Hz over 'time' \n",
        "# seconds (44100 Hz * 2 seconds = 88200)\n",
        "timeSample = np.linspace(0, time, sampleRate * time)\n",
        "timeSample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oItIPoS5cpme"
      },
      "source": [
        "# Generate a sine wave with a frequency of 'frequency' Hz over 'timeSample' samples \n",
        "frequency = 220\n",
        "sineWave = np.sin(2 * np.pi * frequency * timeSample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7InH00ORcsrp"
      },
      "source": [
        "# Configure the sine wave plot\n",
        "plt.figure(figsize=[15, 5])\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title(str(frequency) + ' Hz sine wave')\n",
        "\n",
        "# Only plot 1/4 of the data (0.25 seconds)\n",
        "plt.plot(timeSample[1:(sampleRate // 4)], sineWave[1:(sampleRate // 4)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhG2kwaiEeON"
      },
      "source": [
        "# Generate audio from sine wave data\n",
        "Audio(sineWave, rate=sampleRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y0Er_qM8-WV"
      },
      "source": [
        "**Question**: Would a 440 Hz sine wave have a higher or lower pitch than the 220 Hz sine wave?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2KxY0YnsXw-"
      },
      "source": [
        "### Audification of CO2 concentration data\n",
        "We need to modify our data in order to create an audification. First, our data has a sample rate of 12 samples per year. That sample rate is approximately 0.0000004 Hz, and based on the lower limit of human hearing (20 Hz), this frequency is well below our ability to hear.\n",
        "\n",
        "To bring our dataset into an audible frequency range we must speed it up considerably. We will compress the time scale of our data to a sample rate of 3000 Hz (the lowest frequency at which we can playback audio with the IPython Audio tool). This equates to a frequency increase of about 10^10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMWoQ9LtLfGq"
      },
      "source": [
        "# Sample our data at a rate of 3000 Hz\n",
        "dataSampleRate = 3000\n",
        "\n",
        "# Generate a series of time samples at a rate of 'dataSampleRate' Hz over 1 second\n",
        "timeSample = np.linspace(0, 1, dataSampleRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT1V20UY_vY_"
      },
      "source": [
        "# Use a linear interpolation to fill NaN values–the audio player WILL NOT WORK\n",
        "# with data that contains any NaN values\n",
        "co2Int = co2[\"CO2ppm\"].interpolate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_PnFimTeUAa"
      },
      "source": [
        "# Configure the plot\n",
        "plt.figure(figsize=[8, 5])\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('CO2 concentration compressed to a sample rate of 3000 Hz')\n",
        "\n",
        "# Plot the time compress waveform of CO2 concentration data\n",
        "plt.plot(timeSample[:len(co2Int)], co2Int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm8zm69Wefml"
      },
      "source": [
        "# The sample rate of our data is approximately 0.0000004 Hz (12 samples / year),\n",
        "# we speed it up about 10^10 times (3000 Hz)\n",
        "Audio(co2Int, rate=dataSampleRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88FxaUqO_IGL"
      },
      "source": [
        "**Question:** Why is our audification so short?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivhZJCWnHVie"
      },
      "source": [
        "### Audification of normalized CO2 concentration data\n",
        "Even when resampling our dataset at a higher frequency, we still don't really get a useful audio representation of our waveform. We need to modify our dataset even further to establish a central value about which we can measure displacement. To do this we will normalize our dataset by removing the longterm trend from the data (i.e., subtracting the seasonally adjusted CO2 concentration values from the true CO2 concentration values)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86OvHtAewrOH"
      },
      "source": [
        "# Remove the longterm trend of increasing CO2 concentration\n",
        "co2[\"fit_removed\"] = (co2[\"CO2ppm\"] - co2[\"season_adj\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU3POvVJ_2Ez"
      },
      "source": [
        "# Use a linear interpolation to fill NaN values–the audio player WILL NOT WORK\n",
        "# with data that contains any NaN values\n",
        "co2FitInt = co2[\"fit_removed\"].interpolate()\n",
        "\n",
        "# Print the data\n",
        "co2FitInt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKZvCW4ljJBG"
      },
      "source": [
        "# Plot CO2 concentration data\n",
        "plt.figure(figsize=[15, 10])\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.xlabel('Time (months)')\n",
        "plt.ylabel('CO2 ppm')\n",
        "plt.title('CO2 concentration')\n",
        "plt.plot(co2[\"CO2ppm\"])\n",
        "\n",
        "# Plot seasonally adjusted CO2 concentration data\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.xlabel('Time (months)')\n",
        "plt.ylabel('CO2 ppm')\n",
        "plt.title('Seasonally adjusted CO2 concentration')\n",
        "plt.plot(co2[\"season_adj\"])\n",
        "\n",
        "# Plot normalized CO2 concentration data over 3000 Hz sample frequency\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Normalized CO2 concentration artificially sampled at 3000 Hz')\n",
        "plt.plot(timeSample[:len(co2FitInt)], co2FitInt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAQo4lnjMZz"
      },
      "source": [
        "# The sample rate of our data is approximately 0.0000004 Hz (12 samples / year),\n",
        "# we speed it up about 10^10 times (3000 Hz)\n",
        "Audio(co2FitInt, rate=dataSampleRate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPZf8HifA3lr"
      },
      "source": [
        "**Question:** Does this audification provide you with any insight into the data or help you pick out any discernable patterns?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1KTsWrUQZZ5"
      },
      "source": [
        "## Sonification\n",
        "Sonification is the process of mapping data to properties of sound. As previously mentioned, these properties can include pitch (frequency), amplitude (loudness), tempo (beat speed), and timbre (quality). The spatial position of sound (for example, panning audio to the left or right channel in a stereo mix) can also be used as a mapping property.\n",
        " \n",
        "As opposed to audification, sound property mapping provides more options and opportunities to represent the features of a dataset.\n",
        "\n",
        "In this demonstration we will only explore using variations in pitch, tempo, and timbre to represent monthly atmospheric CO2 values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er5glbZEwrOK"
      },
      "source": [
        "### Subsample CO2 concentration data for sonification\n",
        "We will subsample our data to create a smaller dataset that will be appropriate for generating shorter demo sonifications. Additionally, due to some constraints with audio playback in Google Colab we have to create audio files of our sonifications and then load them into our notebook for playback. In this case, smaller files are preferable for shorter load times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ar1VxrswrOL"
      },
      "source": [
        "# Create a subsample of the CO2 data from the year 2000 to present (co2Modern)\n",
        "co2Modern = co2[co2[\"year\"] >= 2000]\n",
        "\n",
        "# Print the data\n",
        "co2Modern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2opdyLvLdKA"
      },
      "source": [
        "# We are only going to be working with the measured CO2 concentration values\n",
        "# moving forward, so we will store it in a variable (co2ppm)\n",
        "co2ppm = co2Modern[\"CO2ppm\"]\n",
        "\n",
        "# Print the data\n",
        "co2ppm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC4WQzC0wrOP"
      },
      "source": [
        "# Plot the subsampled CO2 concentration data\n",
        "plt.plot(co2ppm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R20VNbUnCOv9"
      },
      "source": [
        "# Check out some basic statistics of the subsampled data\n",
        "co2ppm.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlv18L5UEIiT"
      },
      "source": [
        "**Question:** Given the values of the descriptive statistics, could we directly use the CO2 concentration data as frequencies (in Hz) in a sonification?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uZN64XgW4qI"
      },
      "source": [
        "### Functions for generating audio streams and creating audio files\n",
        "The function `createAudioStream` creates a music 21 audio stream that is playable in local Jupyter notebook using the call `[streamName].show('midi')`. This functionality is not available in Google Colab, so we must also create an audio file for playback using the function `createAudioFile`.\n",
        "\n",
        "`createAudioStream` requires three arguments:\n",
        "- `notes` - a series of music21 Note objects\n",
        "- `bpm` - beats per minute, or the speed of the sonification (default value is 120)\n",
        "- `instrumentName` - the name of a synthesized instrument to play the notes (default value is \"Piano\"). A list of instrument names is available in the [music21 documentation](http://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html#module-music21.instrument)\n",
        "\n",
        "These three arguments provide the data mappings to frequency (the `notes` argument), tempo (the `bpm` argument), and timbre (the `instrumentName` argument)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GGk-mHVXwWD"
      },
      "source": [
        "def createAudioStream(notes, bpm=120, instrumentName=\"Piano\"):\n",
        "  \"\"\"\n",
        "  This function creates and returns a new stream of notes using the music21\n",
        "  Stream class given a series of notes, beats per minute (bpm), and an instrument\n",
        "  name (instrumentName). For a list of valid instrument names see the music21\n",
        "  Instrument module documentation https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n",
        "  \"\"\"\n",
        "  # Create a new music21 stream object to add notes to\n",
        "  newStream = stream.Stream()\n",
        "  # Set the tempo string of the stream\n",
        "  newStream.append(tempo.MetronomeMark(number=bpm))\n",
        "  # Set the instrument to play the stream notes\n",
        "  newStream.append(getattr(instrument, instrumentName)())\n",
        "  # Iterate over the notes provided in the series\n",
        "  for thisNote in notes:\n",
        "    # Append the note to the new stream\n",
        "    newStream.append(thisNote)\n",
        "    # Set the length of the note (4 notes make a beat)\n",
        "    thisNote.seconds = 60 / 4 / bpm\n",
        "  # Return a music21 stream object\n",
        "  return newStream"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkBAJbWCYFOB"
      },
      "source": [
        "def createAudioFile(stream, filename):\n",
        "  \"\"\"\n",
        "  This function writes two audio files given a music21 stream and a filename.\n",
        "  It writes a MIDI file using music21 and then uses the newly written MIDI file\n",
        "  to create the specified audio file using FluidSynth midi to audio conversion\n",
        "  \"\"\"\n",
        "  # Use the music21 stream object write function to create a MIDI file\n",
        "  stream.write(\"midi\", filename.split('.')[0] + \".mid\")\n",
        "  # Use the FluidSynth module to call the sound font and convert the newly \n",
        "  # created MIDI file to the specified filename using the midi_to_audio function\n",
        "  FluidSynth(\"font.sf2\").midi_to_audio(filename.split('.')[0] + \".mid\", filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQcth6LrwrOR"
      },
      "source": [
        "### Sonification using CO2 concentration data values as frequency mapped to pitch\n",
        "In this demonstration we will use a function that converts a data value interpreted as a frequency to a corresponding pitch.\n",
        "\n",
        "For example, a data value of 440 would be interpreted as the frequency 440 Hz. This frequency would then be converted to scientific pitch notation as the pitch A4 (A in octave 4).\n",
        "\n",
        "Moving forward we will be thinking of frequency in terms of Hz and scientific pitch notation. It might be helpful to reference this [table of piano key pitches and frequencies](https://en.wikipedia.org/wiki/Piano_key_frequencies#List)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxC5HdyPYDPC"
      },
      "source": [
        "We will use the defined function `translateValueToPitch` to translate data values as frequency to pitch. This function takes one argument:\n",
        "- `valueAsFrequency` - a data value assumed to be a frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhuk3043wrOU"
      },
      "source": [
        "def translateValueToPitch(valueAsFrequency):\n",
        "  \"\"\"\n",
        "  This function translates raw data values as frequency to a corresponding\n",
        "  pitch. It returns the pitch of the provided frequency falue if the value\n",
        "  is greater than zero. If the value is less than zero it returns a rest (i.e., \n",
        "  silence).\n",
        "  \"\"\"\n",
        "  # Test if the frequency value is greater than zero\n",
        "  if (valueAsFrequency > 0):\n",
        "    # Create a music21 Note object\n",
        "    convNote = note.Note()\n",
        "    # Set the pitch of the Note object based on the supplied frequency value\n",
        "    convNote.pitch.frequency = valueAsFrequency\n",
        "  else:\n",
        "    # Set the Note to a Rest value (silence)\n",
        "    convNote = note.Rest()\n",
        "  # Return the Note object with the assigned pitch\n",
        "  return convNote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnNguBMc4PmW"
      },
      "source": [
        "# Test the translateValueToPitch function\n",
        "testNote = translateValueToPitch(440)\n",
        "testNote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPc2WNAxMGKD"
      },
      "source": [
        "The `translateValueToPitch` function has returned a [music21 Note object](http://web.mit.edu/music21/doc/moduleReference/moduleNote.html#module-music21.note). A Note object has many properties. For this workshop the most important property is the note name. We can get a full desciptive name by calling the property `fullName` on a Note object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaxkfC8FvbRr"
      },
      "source": [
        "# Print out full note name information (fullName)\n",
        "testNote.fullName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80rVTGkawrOW"
      },
      "source": [
        "# Map CO2 concentration values directly to pitch\n",
        "rawPitch = co2ppm.apply(translateValueToPitch)\n",
        "\n",
        "#Print the data\n",
        "rawPitch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP72yepSwrOY"
      },
      "source": [
        "# Create an audio stream of the rawPitch values using the createAudioStream function\n",
        "# Pass in the series of notes (rawPitch), a tempo in bpm, and an instrument name\n",
        "# from the list located in the right-hand column of this music21 documentation\n",
        "# page: https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n",
        "rawPitchStream = createAudioStream(rawPitch, 100, 'Xylophone')\n",
        "\n",
        "# If running on a local machine, uncomment the line below to play the MIDI data\n",
        "# in the Jupyter notebook without having to create a file\n",
        "# rawPitchStream.show('midi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Omr_loyux6"
      },
      "source": [
        "In the next cell we are creating an audio file from the music21 stream we created. We will create a FLAC audio file, but it is also possible to create other standard audio file formats such as mp3, mp4, and wave files by changing the file extension. When running this notebook in Colab I found that FLAC audio files tended to load faster with than other file types when using the Audio tool. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ66bB6UU316"
      },
      "source": [
        "# Create an audio file from the rawPitchStream\n",
        "createAudioFile(rawPitchStream, 'rawPitch.flac')\n",
        "\n",
        "# Load the newly created audio file for playback\n",
        "Audio('rawPitch.flac')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ0tcDEeyoGy"
      },
      "source": [
        "# Plot the data for visual reference\n",
        "plt.plot(co2ppm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbMeBGDjTJ6d"
      },
      "source": [
        "**Activity:** Create a sonification of the seasonally adjusted CO2 concentration data using the `translateValueToPitch` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UH95jGTTa-P"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7slkEh5YYBg"
      },
      "source": [
        "**Question:** Are there any shortcomings with the method we used to create this sonification?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rueqdiqmwrOc"
      },
      "source": [
        "### Sonification using values mapped to linear pitch range\n",
        "In this demonstration we will use a function that maps a data value from the dataset range (dataMin - dataMax) to the pitch range of a 97 key piano (~16 Hz – ~4186 Hz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6F2Nh9Igz9U"
      },
      "source": [
        "We will use the defined function `mapValueToPitchRange` to map data values from the data domain to a pitch range. This function takes two arguments:\n",
        "- `data` - the data to be mapped to the new pitch range\n",
        "- `dataMinMax` - a list containing the min and max of the dataset ([min, max])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z93Dh_EEwrOt"
      },
      "source": [
        "def mapValueToPitchRange(data, dataMinMax):\n",
        "  \"\"\"\n",
        "  This function maps a data value from the dataset domain (data min - data max)\n",
        "  to the frequency range of a standard 97 key piano (~16 Hz – ~4186 Hz). It\n",
        "  returns a music21 Note object with the pitch of the mapped frequency value if\n",
        "  the value is greater than zero. If the value is less than zero it returns a\n",
        "  Note object with a pitch frequency of 1. \n",
        "  \"\"\"\n",
        "  convNote = note.Note()\n",
        "  dataRange = dataMinMax[1] - dataMinMax[0]\n",
        "  # Human hearing range: ~20Hz - ~20000Hz\n",
        "  MIN_HZ = 16.35\n",
        "  MAX_HZ = 4186.01\n",
        "  convNote.pitch.frequency = (\n",
        "    ((data - dataMinMax[0]) * (MAX_HZ - MIN_HZ)) / dataRange\n",
        "  ) + MIN_HZ\n",
        "  return convNote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVUWtlLn8GdE"
      },
      "source": [
        "# Test the mapValueToPitchRange function\n",
        "mapValueToPitchRange(50, [0, 100]).fullName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhEaI1pawrOv"
      },
      "source": [
        "# Calculate the minimum and maximum values of the CO2 concentration data\n",
        "co2Range = co2ppm.agg([\"min\", \"max\"])\n",
        "\n",
        "# Convert values to pitch range\n",
        "pitchRange = co2ppm.apply(\n",
        "  lambda x: mapValueToPitchRange(x, [co2Range[\"min\"], co2Range[\"max\"]])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzKaNhYIwrOy"
      },
      "source": [
        "# Create an audio stream of the pitchRange values using the createAudioStream function\n",
        "# Pass in the series of notes (pitchRange), a tempo in bpm, and an instrument name\n",
        "# from the list located in the right-hand column of this music21 documentation\n",
        "# page: https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n",
        "pitchRangeStream = createAudioStream(pitchRange, 80, 'Glockenspiel')\n",
        "\n",
        "# If running on a local machine, uncomment the line below to play the MIDI data\n",
        "# in the Jupyter notebook without having to create a file\n",
        "# pitchRangeStream.show('midi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NAe1HKfe6HG"
      },
      "source": [
        "# Create an audio file from the pitchRangeStream\n",
        "createAudioFile(pitchRangeStream, 'pitchRange.flac')\n",
        "\n",
        "# Load the newly created audio file for playback\n",
        "Audio('pitchRange.flac')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zuCH3KmdvTp"
      },
      "source": [
        "# Plot the actual concentration data and the mapped frequency data for visual reference\n",
        "plt.figure(figsize=[12, 5])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"CO2 concentration over time\")\n",
        "plt.plot(co2ppm)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"CO2 concentration mapped to frequency over time\")\n",
        "plt.plot(pitchRange.apply(lambda x: x.pitch.frequency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTXDeWoiVGTJ"
      },
      "source": [
        "**Activity:** Create a sonification of the seasonally adjusted CO2 concentration data using the `mapValueToPitchRange` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkfjRYl9VF7N"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvZvi4i-cnpn"
      },
      "source": [
        "### Sonification using values mapped to logarithmic pitch range\n",
        "The frequency ranges between pitches are not linear. For example, the frequency of note A4 is 440 Hz. To get to the next octave of this note we must double the frequency of A4, giving A5 as 880 Hz (i.e., A6 = 1760, A3 = 220, and so on). In order to map our values to an appropriate pitch scale, we can apply a log base two transform to pitch range. This makes it easier to discern pitches at lower frequencies and produces a more natural and pleasing sound.\n",
        "\n",
        "In this demonstration we will use a function that maps a data value from the dataset range (dataMin - dataMax) to the log2 frequency range of a 97 key piano (log2(\\~16 Hz) – log2(\\~4186 Hz))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaYvm_QxhTXr"
      },
      "source": [
        "We will use the defined function `mapValueToPitchRangeExpScale` to map data values from the data domain to a log scale pitch range. This function takes two arguments:\n",
        "- `data` - the data to be mapped to the new pitch range\n",
        "- `dataMinMax` - a list containing the min and max of the dataset ([min, max])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvu14U2ewrOm"
      },
      "source": [
        "# https://stackoverflow.com/questions/19472747/convert-linear-scale-to-logarithmic\n",
        "#           x - x0\n",
        "# log(y) = ------- * (log(y1) - log(y0)) + log(y0)\n",
        "#          x1 - x0\n",
        "def mapValueToPitchRangeExpScale(data, dataMinMax):\n",
        "  \"\"\"\n",
        "  This function maps a data value from the dataset domain (data min - data max)\n",
        "  to the log2 frequency range of a standard 97 key piano (~16 Hz – ~4186 Hz). It\n",
        "  returns a music21 Note object with the pitch of the mapped frequency value if\n",
        "  the data value is greater than zero. If the data value is less than zero it\n",
        "  returns a Note object with a pitch frequency of 1. \n",
        "  \"\"\"\n",
        "  convNote = note.Note()\n",
        "  dataRange = dataMinMax[1] - dataMinMax[0]\n",
        "  # Human hearing range: ~20Hz - ~20000Hz\n",
        "  MIN_HZ = 16.35\n",
        "  MAX_HZ = 4186.01\n",
        "  convNote.pitch.frequency = np.exp2(\n",
        "      (data - dataMinMax[0]) / dataRange *\n",
        "      (np.log2(MAX_HZ) - np.log2(MIN_HZ)) +\n",
        "      np.log2(MIN_HZ)\n",
        "  )\n",
        "  return convNote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBK8oSmrTAp2"
      },
      "source": [
        "# Test the mapValueToPitchRange function\n",
        "mapValueToPitchRangeExpScale(0, [-50, 50]).fullName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nYRAPzkwrOo"
      },
      "source": [
        "# Convert values to pitch range\n",
        "pitchRangeExp = co2ppm.apply(lambda x:\n",
        "  mapValueToPitchRangeExpScale(x, [co2Range[\"min\"], co2Range[\"max\"]])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfnHYcCCwrOq"
      },
      "source": [
        "# Create an audio stream of the pitchRangeExp values using the createAudioStream\n",
        "# function. Pass in the series of notes (pitchRangeExp), a tempo in bpm, and an\n",
        "# instrument name from the list located in the right-hand column of this music21\n",
        "# documentation page: https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n",
        "pitchRangeExpStream = createAudioStream(pitchRangeExp, 100, 'Xylophone')\n",
        "\n",
        "# If running on a local machine, uncomment the line below to play the MIDI data\n",
        "# in the Jupyter notebook without having to create a file\n",
        "# pitchRangeExpStream.show('midi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7eFc7kufKed"
      },
      "source": [
        "# Create an audio file from the pitchRangeExpStream\n",
        "createAudioFile(pitchRangeExpStream, 'pitchRangeExp.flac')\n",
        "\n",
        "# Load the newly created audio file for playback\n",
        "Audio('pitchRangeExp.flac')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFlUrzq_b_MI"
      },
      "source": [
        "# Plot the actual concentration data and the mapped frequency data for visual reference\n",
        "plt.figure(figsize=[12, 5])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"CO2 concentration over time\")\n",
        "plt.plot(co2ppm)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"CO2 concentration mapped to frequency over time\")\n",
        "plt.plot(pitchRangeExp.apply(lambda x: x.pitch.frequency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGwGe_QHVcNZ"
      },
      "source": [
        "**Activity:** Create a sonification of the seasonally adjusted CO2 concentration data using the `mapValueToPitchRangeExpScale` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qYPT9xgVb7r"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7DhEXAXwrO3"
      },
      "source": [
        "### Sonification using values mapped to a musical scale\n",
        "In this demonstration we will use a function that maps a data value from the dataset range (dataMin - dataMax) to a specified [musical scale](https://en.wikipedia.org/wiki/Scale_(music)) with the given root note (e.g., an ordered group of pitches) and an octave range.\n",
        "\n",
        "Using a musical scale provides us with some options to make our sonification more \"musical\". Previously we were mapping values to frequencies and pitches without consideration for ordering of notes. Predefined musical scales can evoke culturally influenced emotions that can give our sonification an aesthetic feeling. For example, major scales are often interpreted as happy, while minor scales are interpreted as sad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQblzbv6h8AN"
      },
      "source": [
        "We will use the defined function `mapValueToScale` to map data values from the data domain to a musical scale. This function takes five arguments:\n",
        "- `data` - the data to be mapped to the new pitch range\n",
        "- `dataMinMax` - a list containing the min and max of the dataset ([min, max])\n",
        "- `tonic` - a string containing a valid scientific pitch notation for the root note of the scale\n",
        "- `scaleName` - a string containing a valid scale subclass name from the music21 scale class (default value is \"MajorScale\"). A list of scale names is available in the [music21 documentation](http://web.mit.edu/music21/doc/moduleReference/moduleScale.html#module-music21.scale)\n",
        "- `octaveRange` - a list containing the min and max octaves over which to map the pitches (default value is [3, 5])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TIcOeWuwrO3"
      },
      "source": [
        "def mapValueToScale(data, dataMinMax, tonic=\"c\", scaleName=\"MajorScale\",\n",
        "  octaveRange=[3, 5]):\n",
        "  \"\"\"\n",
        "  This function maps a data value from the dataset domain (data min - data max)\n",
        "  to a musical scale with a provided tonic note over a provided octave range. It\n",
        "  returns a music21 Note object with the pitch of the mapped data value if the\n",
        "  data value is greater than zero. If the data value is less than zero it\n",
        "  returns a Note object with a pitch frequency of 1. \n",
        "  \"\"\"\n",
        "  convNote = note.Note()\n",
        "  sc1 = getattr(scale, scaleName)(tonic)\n",
        "  dataRange = dataMinMax[1] - dataMinMax[0]\n",
        "  SCALE_RANGE = len(sc1.getPitches(tonic + str(octaveRange[0]),tonic + str(octaveRange[1]))) - 1\n",
        "  pos = np.round(((data - dataMinMax[0]) * SCALE_RANGE) / dataRange)\n",
        "  convNote.pitch = sc1.getPitches(tonic + str(octaveRange[0]),tonic + str(octaveRange[1]))[int(pos)]\n",
        "  return convNote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9WZxcng2VQB"
      },
      "source": [
        "The C major scale has a root note (tonic) of C and consists of the notes: C, D, E, F, G, A, B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKNHwhlN2GjZ"
      },
      "source": [
        "# Test the mapValueToScale function\n",
        "cMajorD = mapValueToScale(1, [0, 7], octaveRange=[3,4])\n",
        "cMajorD.fullName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGPC7-mdwrO5"
      },
      "source": [
        "# Convert values to musical scale\n",
        "musicalNotes = co2ppm.apply(lambda x:\n",
        "  mapValueToScale(\n",
        "    x,\n",
        "    [co2Range[\"min\"], co2Range[\"max\"]],\n",
        "    tonic=\"c\",\n",
        "    scaleName=\"MinorScale\",\n",
        "    octaveRange=[1, 4]\n",
        "  )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "5UvzyVlawrO7"
      },
      "source": [
        "# Create an audio stream of the musicalNotes values using the createAudioStream\n",
        "# function. Pass in the series of notes (musicalNotes), a tempo in bpm, and an\n",
        "# instrument name from the list located in the right-hand column of this music21\n",
        "# documentation page: https://web.mit.edu/music21/doc/moduleReference/moduleInstrument.html\n",
        "musicalNotesStream = createAudioStream(musicalNotes, 100, 'Kalimba')\n",
        "\n",
        "# If running on a local machine, uncomment the line below to play the MIDI data\n",
        "# in the Jupyter notebook without having to create a file\n",
        "# musicalNotesStream.show('midi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJiYGjeDdyxo"
      },
      "source": [
        "# Create an audio file from the musicalNotesStream\n",
        "createAudioFile(musicalNotesStream, 'musicalNotes.flac')\n",
        "\n",
        "# Load the newly created audio file for playback\n",
        "Audio('musicalNotes.flac')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfyDmgPhwrO9"
      },
      "source": [
        "# Plot the actual concentration data and the mapped frequency data for visual reference\n",
        "plt.figure(figsize=[12, 5])\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"CO2 concentration over time\")\n",
        "plt.plot(co2ppm)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"CO2 concentration mapped to frequency over time\")\n",
        "plt.plot(musicalNotes.apply(lambda x: x.pitch.frequency))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MSLfRTVVm7p"
      },
      "source": [
        "**Activity:** Create a sonification of the seasonally adjusted CO2 concentration data using the `mapValueToScale` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CxWBLuHVmrf"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA_c8TCS4ZiU"
      },
      "source": [
        "### Sonification playground\n",
        "Use the cells below to generate your own sonifications of the CO2 concentration data using the provided functions.\n",
        "\n",
        "You can the call the following functions on a Pandas Series using the `apply()` function:\n",
        "- `translateValueToPitch(valueAsFrequency)`\n",
        "- `mapValueToPitchRange(data, dataMinMax)`\n",
        "- `mapValueToPitchRangeExpScale(data, dataMinMax)`\n",
        "- `mapValueToScale(data, dataMinMax, tonic=\"c\", scaleName=\"MajorScale\",\n",
        "  octaveRange=[3, 5])`\n",
        "\n",
        "For example, to create a sonification of the modern CO2 data you could do the following:\n",
        "\n",
        "1. Map the data to a log scale pitch range to return a series of music21 Note objects\n",
        "```python\n",
        "pitchRange = co2Modern[\"season_adj\"].apply(lambda x:\n",
        "      mapValueToPitchRangeExpScale(x, [co2Range[\"min\"], co2Range[\"max\"]])\n",
        ")\n",
        "```\n",
        "1. Create a music21 stream of the Note objects at 100 bpm using a Guitar sound\n",
        "```python\n",
        "notesStream = createAudioStream(pitchRange, 100, 'Guitar')\n",
        "```\n",
        "1. Create an audio file from the stream and play it back\n",
        "```python\n",
        "createAudioFile(notesStream, 'demoNotes.flac')\n",
        "Audio('demoNotes.flac')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4qUFuuV4YzG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX2r4AxJH8fI"
      },
      "source": [
        "## Other sonification resources\n",
        "- [Sonic Pi](https://sonic-pi.net/) - a code-based music creation and performance tool based on the Ruby programming language\n",
        "- [p5.js](https://p5js.org/) - a JavaScript library for creative coding that includes a library for interfacing with web-based audio.\n",
        "- [Web audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) - base API for controling audio on the web\n",
        "- [SAS Graphics Accelerator](https://support.sas.com/software/products/graphics-accelerator/samples/index.html) - a free Chrome extension that allows you to sonify data captured from web tables and data from web-based SAS visualization\n",
        "- [TwoTone](https://app.twotone.io/) - An interactive web application for easily creating sonifications with uploaded data\n",
        "- [Loud Numbers](https://www.loudnumbers.net/) - a data sonification podcast and mailing list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU0SuSO1wrPO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}